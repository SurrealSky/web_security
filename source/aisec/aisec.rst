内生安全
========================================

基础
----------------------------------------
人工智能内生安全就是AI系统的本身安全，AI系统现在存在脆弱性，包括框架组件，数据，算法，模型等。

风险分析
----------------------------------------

安全框架
----------------------------------------

对抗攻击
----------------------------------------

投毒攻击
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
该攻击通过修改一定数量的训练数据使模型训练过程产生错误的关系输出。该攻击发生在训练阶段，攻击目标是训练数据集。攻击者具有获取、修改或创造训练数据集的能力，知道训练数据集的标签等背景知识。

对抗样本攻击
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
该攻击通过构造对抗样本，使模型推测过程产生错误结果。攻击发生在推理阶段，攻击目标是测试数据集。攻击者具有获取和修改测试样本的能力，知道标签等背景知识。

数据窃取攻击
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
该攻击通过存储和通信机制的漏洞、查询或反演技术等多种手段，窃取机器学习隐私信息(如：训练数据、模型训练方法和训练参数)。该攻击大部分发生在黑盒攻击中，攻击者仅具有窃取部分数据的能力。

隐私询问攻击
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
指攻击者在无法获取训练数据和模型数据情况下，仅通过观察测试数据输入模型后返回的结果即询问结果的方式实施隐私信息的计算和推测。

成员推理攻击
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
指攻击者根据询问结果判断出某个个体是否参与模型训练。训练数据提取攻击：指攻击者利用询问数据与已有知识推测训练数据隐私的攻击。模型提取攻击：指攻击者利用询问接口获得模型的分类与测试输入输出数据，从而重构一个与原模型相似的模型的攻击。 

对抗防御
----------------------------------------

正则化
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
指通过对训练数据和模型规范化操作，降低模型的出错率。训练数据的正则化可以防御训练数据投毒攻击，提高模型泛化能力，主要包括数据集增强、数据集扩充等措施;模型的正则化可以防御对抗样本攻击,主要利用正则化项对模型参数和训练方式进行规范化处理。

对抗训练
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
指使用对抗模型生成带有完全标注的对抗样本和合法样本混合起来对原模型进行训练的过程。主要目的是学习对抗样本和正确标签的关系，提升模型鲁棒性。

防御精馏
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
指通过一个模型的输出训练另一个模型的机器学习算法，是在保证训练精度条件下压缩模型方法，可以增强模型面对对抗样本攻击的鲁棒性。

隐私保护机制
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
数据窃取攻击和隐私询问攻击主要针对模型和数据的隐私。加密、扰动等机制可以保护数据和模型的隐私。分布式机器学习、差分隐私等机制可以使模型以隐私保护的方式进行学习。